{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3I1SFUTkJMy"
   },
   "source": [
    "**********************************************************************************************\n",
    "# Initialization\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xl7e8up8GlLJ"
   },
   "outputs": [],
   "source": [
    "# Load librarie\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1576210350678,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "enGW7f2skLkl",
    "outputId": "e7669c8a-d7cb-417f-d050-2f9112c30617"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/bassamarnaout/Library/CloudStorage/GoogleDrive-bassamarnaout2@gmail.com/My Drive/DataMining/Final_Project/DataMiningFinalProject-All.ipynb Cell 3\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bassamarnaout/Library/CloudStorage/GoogleDrive-bassamarnaout2%40gmail.com/My%20Drive/DataMining/Final_Project/DataMiningFinalProject-All.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# ''' Remove this comment out if needs to be used\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bassamarnaout/Library/CloudStorage/GoogleDrive-bassamarnaout2%40gmail.com/My%20Drive/DataMining/Final_Project/DataMiningFinalProject-All.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bassamarnaout/Library/CloudStorage/GoogleDrive-bassamarnaout2%40gmail.com/My%20Drive/DataMining/Final_Project/DataMiningFinalProject-All.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2018,
     "status": "ok",
     "timestamp": 1576210355994,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "Nsl2xtoNkYqz",
    "outputId": "73b7f932-325f-4d36-afca-054a5673b000"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "import os\n",
    "##############################################################################\n",
    "\n",
    "os.chdir(\"/content/drive/My Drive/DataMining/Final_Project\")\n",
    "!ls\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PuLVSoLGWWm"
   },
   "source": [
    "**********************************************************************************************\n",
    "# Functions\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1kW9wdzwWX4"
   },
   "source": [
    "ROC Curve plot function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUXTZ2-gGrl0"
   },
   "source": [
    "This is a callable ROC curve plot function. We will use this function to plot ROC Curve for all the models. We have used Seaborn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJoVHVQWGbbR"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set('talk', 'whitegrid', 'dark', font_scale=1,rc={\"lines.linewidth\": 2, 'grid.linestyle': '--'})\n",
    "def plotAUC(truth, pred, lab):\n",
    "    fpr, tpr, _ = metrics.roc_curve(truth,pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    lw = 2\n",
    "    c = (np.random.rand(), np.random.rand(), np.random.rand())\n",
    "    plt.plot(fpr, tpr, color= c,lw=lw, label= lab +'(AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve') #Receiver Operating Characteristic \n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "def plotRocCurve(pred,truth, lab):\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  # AUC Curve\n",
    "  # y_pred_proba = classifier.predict_proba(pred)[:,1]\n",
    "  fpr, tpr, thresholds = metrics.roc_curve(truth,pred) \n",
    "  auc = metrics.auc(fpr, tpr)#metrics.roc_auc_score(fpr, tpr)\n",
    "  plt.plot(fpr,tpr,label= lab + \", auc=\"+str(auc))\n",
    "  plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.title('Receiver operating characteristic (ROC) ')\n",
    "  plt.show()\n",
    "\n",
    "  print('\\nArea under ROC curve is (AUC) :',str(format(auc,'.6f') ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKa1-nX_wac2"
   },
   "source": [
    "Confusion Matrix Viz function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEm8mMHGwmYG"
   },
   "source": [
    "This is a callable Confusion Matrix Visualization function. We have used this function to visualize True positives, True Negatives, False Positives and False Negatives for all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTfwjVI4wsT6"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(Y,Ypred, normalize=False): # This function prints and plots the confusion matrix.\n",
    "    cm = confusion_matrix(Y, Ypred, labels=[0, 1])\n",
    "    classes=[\"Will Pay\", \"Will Default\"]\n",
    "    cmap = plt.cm.Blues\n",
    "    title = \"Confusion Matrix\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.around(cm, decimals=3)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lJxh9RXmQsz"
   },
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "   diagonal_sum = confusion_matrix.trace()\n",
    "   sum_of_all_elements = confusion_matrix.sum()\n",
    "   return diagonal_sum / sum_of_all_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qAy90qskmOF"
   },
   "source": [
    "**********************************************************************************************\n",
    "# 1.load data\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1138,
     "status": "ok",
     "timestamp": 1576210368720,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "aZDnOfYdkoCU",
    "outputId": "e8846d98-1e70-4d6b-a867-876786330187"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "##############################################################################\n",
    " \n",
    "data = pd.read_pickle('dataset.pkl')\n",
    "\n",
    "\n",
    "data.columns = ['id', 'dates', 'transaction_amount', 'days_before_request',\n",
    "                       'loan_amount', 'loan_date', 'isDefault']\n",
    "\n",
    "# drop all those rows which  \n",
    "# have any 'nan' value in it. \n",
    "data.dropna(inplace = True) \n",
    "\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\n')\n",
    "print('Bank Customer Transactions Dataset:\\n')\n",
    "# print(data.head())\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qchIFLaVaXf"
   },
   "source": [
    "## Formulate Data to new prospective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1576210937564,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "1Cebm9OOKWUt",
    "outputId": "76546b09-24e9-458b-aff4-28ce1b40ff8b"
   },
   "outputs": [],
   "source": [
    "x_data = np.zeros([15000,6])\n",
    "y_label = np.zeros(15000)\n",
    "\n",
    "cust_id_list = []\n",
    "\n",
    "for cust in np.arange(0,15000):\n",
    "  cust_id_list.append(data['id'][cust][1])\n",
    "  ts = data['transaction_amount'][cust][1]\n",
    "  sum_ts = np.sum(ts)\n",
    "  loan_amt = data['loan_amount'][cust][1]\n",
    "\n",
    "  if cust <=9999:\n",
    "    isdef = data['isDefault'][cust][1]\n",
    "    y_label[cust] = isdef\n",
    "    # print(y_label)\n",
    "\n",
    "  x_data[cust,0] = np.mean(ts[np.where(ts>0)])\n",
    "  x_data[cust,1] = np.mean(ts[np.where(ts<0)])\n",
    "  x_data[cust,2] = sum_ts\n",
    "  x_data[cust,3] = np.max(ts)\n",
    "  x_data[cust,4] = np.min(ts)\n",
    "  x_data[cust,5] = loan_amt\n",
    "\n",
    "\n",
    "dataframe1 = pd.DataFrame(np.array(cust_id_list), columns = ['id'])\n",
    "\n",
    "coulumsName = ['mean_+ts', 'mean_-ts', 'sum_ts', 'max_ts',\n",
    "                       'min_ts', 'loan_amount']\n",
    "dataframe2 = pd.DataFrame.from_records(x_data, columns = coulumsName)\n",
    "dataframe2['isDefault'] = y_label\n",
    "dataframe2\n",
    "\n",
    "# Place the DataFrames side by side\n",
    "data_formulated = pd.concat([dataframe1, dataframe2], axis=1)\n",
    "data_formulated = data_formulated.drop('id', axis=1)\n",
    "\n",
    "\n",
    "# drop all those rows which  \n",
    "# have any 'nan' value in it. \n",
    "# data_formulated.dropna(inplace = True) \n",
    "data_formulated.fillna(data_formulated.mean(),inplace = True)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\n')\n",
    "print('New Formulated Data For Bank Customer Transactions Dataset:\\n')\n",
    "data_formulated\n",
    "\n",
    "data_to_use = data_formulated.head(10000)\n",
    "data_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRtxWp7IlXLk"
   },
   "source": [
    "**********************************************************************************************\n",
    "# 2.Summarize Data\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46pKcZiKluF8"
   },
   "source": [
    "**********************************************************************************************\n",
    "## 2.A Understand the Data With Descriptive Statistics\n",
    "**********************************************************************************************\n",
    "We must understand the data in order to get the best results. We will discover 7 recipes that we can use in Python to better understand our machine learning data. \n",
    "1. Take a peek at our raw data.\n",
    "2. Review the dimensions of our dataset.\n",
    "3. Review the data types of attributes in our data.\n",
    "4. Summarize the distribution of instances across classes in our dataset.\n",
    "5. Summarize our data using descriptive statistics.\n",
    "6. Understand the relationships in our data using correlations.\n",
    "7. Review the skew of the distributions of each attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZj-MwZel0js"
   },
   "source": [
    "2.A.1. Peek at our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1576208143640,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "v8q2aXEdl1Js",
    "outputId": "007e15a3-aef1-4dca-b893-d51443e49ff0"
   },
   "outputs": [],
   "source": [
    "#2.A.1. Peek at our Data\n",
    "#review the first 20 rows\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\n')\n",
    "print('New Formulated Data For Bank Customer Transactions Dataset:\\n')\n",
    "peek = data_to_use.head(5)\n",
    "peek\n",
    "# We can confirm that the scales for the attributes are all over the place \n",
    "# because of the differing units. We may benefit from some transforms later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKKE0qM_mEvI"
   },
   "source": [
    "2.A.2.Dimensions of Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1576208147663,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "LcUNf4n4mFc4",
    "outputId": "46c5e699-8c29-4028-8713-4c78e55b1144"
   },
   "outputs": [],
   "source": [
    "#2.A.2.Dimensions of Our Data\n",
    "print(data_to_use.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9HHWYhutmPFy"
   },
   "source": [
    "2.A.3.Data Type For Each Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1576207605742,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "bDqRddxSmP36",
    "outputId": "09b918c9-f83b-4054-8cad-43b0491fa566"
   },
   "outputs": [],
   "source": [
    "#2.A.3.Data Type For Each Attribute\n",
    "# types = data_to_use.dtypes\n",
    "# print(types)\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\n')\n",
    "print('Data Type:\\n')\n",
    "data_to_use.info()\n",
    "\n",
    "# We can see that all of the attributes are numeric of integers (int) type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mk7-Mj2mmZYb"
   },
   "source": [
    "2.A.4.Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1576207613451,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "MuRT6LDVmdA3",
    "outputId": "2ddc80ed-a433-4cdb-e971-6d928658e599"
   },
   "outputs": [],
   "source": [
    "#2.A.4.Descriptive Statistics\n",
    "# Descriptive statistics can give us great insight into the shape of each attribute. Often we can\n",
    "# create more summaries than we have time to review. The describe() function on the Pandas\n",
    "# DataFrame lists 8 statistical properties of each attribute. They are:\n",
    "#  Count.\n",
    "#  Mean.\n",
    "#  Standard Deviation.\n",
    "#  Minimum Value.\n",
    "#  25th Percentile.\n",
    "#  50th Percentile (Median).\n",
    "#  75th Percentile.\n",
    "#  Maximum Value.\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\n')\n",
    "print('Descriptive Statistics For Bank Customer Transactions Dataset:\\n')\n",
    "pd.set_option('display.width', 100)\n",
    "# pd.set_option('precision', 3)\n",
    "description = data_to_use.describe()\n",
    "description\n",
    "\n",
    "# We now have a better feeling for how different the attributes are. The min and max values\n",
    "# as well as the means vary a lot. We are likely going to get better results by rescaling the data\n",
    "# in some way.\n",
    "\n",
    "# Data has interestingly differing mean values.\n",
    "# There may be some benefit from standardizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lo94olFpS3r3"
   },
   "source": [
    "2.A.5.Class Distribution (Classi\f\n",
    "cation Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1576207619954,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "PBZHw6VrS5Ai",
    "outputId": "319dfe81-34eb-4260-c115-bcd423c188d3"
   },
   "outputs": [],
   "source": [
    "# 2.A.5.Class Distribution (Classi\f\n",
    "cation Only)\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "# print('\\n')\n",
    "print('Class Distribution:\\n')\n",
    "class_counts = data_to_use.groupby('isDefault').size()\n",
    "print(class_counts)\n",
    "\n",
    "# We can see that the classes are reasonably balanced between output values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5oY34h9T6KI"
   },
   "source": [
    "2.A.6.Correlations Between Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43307,
     "status": "ok",
     "timestamp": 1576086465142,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "cDPMiYYST7NO",
    "outputId": "b37d5bf2-a1f7-4435-aa5f-f3d5779d0baf"
   },
   "outputs": [],
   "source": [
    "# 2.A.6.Correlations Between Attributes\n",
    "# Correlation refers to the relationship between two variables and how they may or may not\n",
    "# change together. The most common method for calculating correlation is Pearson's Correlation\n",
    "# Coe\u000ecient, that assumes a normal distribution of the attributes involved. A correlation of -1\n",
    "# or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no\n",
    "# correlation at all. Some machine learning algorithms like linear and logistic regression can su\u000b\n",
    "ffer\n",
    "# poor performance if there are highly correlated attributes in your dataset. As such, it is a good\n",
    "# idea to review all of the pairwise correlations of the attributes in our dataset. We can use the\n",
    "# corr() function on the Pandas DataFrame to calculate a correlation matrix.\n",
    "\n",
    "# Pairwise Pearson correlations\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\n')\n",
    "print('Correlations Marix Between Attributes :\\n ')\n",
    "correlations = data_to_use.corr(method='pearson')\n",
    "correlations\n",
    "\n",
    "# This is interesting. We can see that many of the attributes have a weak correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1C-yPmp3UNE_"
   },
   "source": [
    "2.A.7.Skew of Univariate Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43235,
     "status": "ok",
     "timestamp": 1576086465145,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "zOXDJCdBUOKN",
    "outputId": "231edb82-804d-4379-ea76-e28475d6e939"
   },
   "outputs": [],
   "source": [
    "# 2.A.7.Skew of Univariate Distributions\n",
    "# Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or\n",
    "# squashed in one direction or another. Many machine learning algorithms assume a Gaussian\n",
    "# distribution. Knowing that an attribute has a skew may allow you to perform data preparation\n",
    "# to correct the skew and later improve the accuracy of your models. We can calculate the skew\n",
    "# of each attribute using the skew() function on the Pandas DataFrame.\n",
    "\n",
    "# Skewness refers to distortion or asymmetry in a symmetrical bell curve, \n",
    "# or normal distribution, in a set of data. If the curve is shifted to the \n",
    "# left or to the right, it is said to be skewed. Skewness can be quantified \n",
    "# as a representation of the extent to which a given distribution varies \n",
    "# from a normal distribution\n",
    "\n",
    "# Skew for each attribute\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\n')\n",
    "print('Skew of Univariate Distributions:\\n ')\n",
    "skew = data_to_use.skew()\n",
    "print(skew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAMEsazoUbDS"
   },
   "source": [
    "**********************************************************************************************\n",
    "## 2.B Understand the Data With Visualization\n",
    "**********************************************************************************************\n",
    "We must understand the data in order to get the best results from machine learning algorithms.\n",
    "The fastest way to learn more about the data is to use data visualization.\n",
    "\n",
    "plot the data using:\n",
    "\n",
    "**Univariate Plots**\n",
    "1.   Histograms.\n",
    "2.   Density Plots.\n",
    "3.   Box and Whisker Plots.\n",
    "\n",
    "**Multivariate Plots**\n",
    "4.   Correlation Matrix Plot.\n",
    "5.   Scatter Plot Matrix.\n",
    "  \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__JARua3Ular"
   },
   "source": [
    "**********************************************************************************************\n",
    "### (2.B.1)Univariate Plots\n",
    "**********************************************************************************************\n",
    "1.   Histograms.\n",
    "2.   Density Plots.\n",
    "3.   Box and Whisker Plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agUMkyeUUq1S"
   },
   "source": [
    "**Histograms**\n",
    "\n",
    "A fast way to get an idea of the distribution of each attribute is to look at histograms. Histograms\n",
    "group data into bins and provide us a count of the number of observations in each bin. From\n",
    "the shape of the bins we can quickly get a feeling for whether an attribute is Gaussian, skewed\n",
    "or even has an exponential distribution. It can also help us see possible outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44869,
     "status": "ok",
     "timestamp": 1576086466855,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "eYQU8tiXUmXa",
    "outputId": "2c184c57-e229-457e-c52b-77f9b4a9d57d"
   },
   "outputs": [],
   "source": [
    "# Univariate Histograms\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\n')\n",
    "print('Univariate Histograms:\\n ')\n",
    "data_to_use.hist(figsize=(15,15))\n",
    "plt.show()\n",
    "\n",
    "# We can see that no attributes  has an exponential\n",
    "# distribution. We can also see that perhaps some attributes\n",
    "# have a Gaussian or nearly Gaussian distribution. This is interesting because many machine learning\n",
    "# techniques assume a Gaussian univariate distribution on the input variables.\n",
    "# This is useful to note as we can use algorithms that can exploit this assumption (Gaussian distribution).\n",
    "\n",
    "#Some has binomial distribution such as Gender\n",
    "\n",
    "#It also looks like some attributes may be skewed Gaussian distributions, which\n",
    "# might be helpful later with transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2al_-MARWhwu"
   },
   "source": [
    "Density Plots\n",
    "Density plots are another way of getting a quick idea of the distribution of each attribute. The plots look like an abstracted histogram with a smooth curve drawn through the top of each bin, much like your eye tried to do with the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47805,
     "status": "ok",
     "timestamp": 1576086469872,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "1DkkYFnGUeGI",
    "outputId": "e7c29888-1cfa-4f14-f15f-53fea4a1a879"
   },
   "outputs": [],
   "source": [
    "# Univariate Density Plots\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\n')\n",
    "print('Univariate Density Plots:\\n ')\n",
    "data_to_use.plot(kind='density', subplots=True, layout=(3,3), sharex=False)\n",
    "plt.show()\n",
    "\n",
    "# This is useful, you can see that some of the attributes have a skewed distribution. A power\n",
    "# transform like a Box-Cox transform that can correct for the skew in distributions might be\n",
    "# useful.\n",
    "\n",
    "# This helps point out the skew in many distributions so much so that data looks like outliers\n",
    "# (e.g. beyond the whisker of the plots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5f5_RP2XBp0"
   },
   "source": [
    "**Box and Whisker Plots**\n",
    "\n",
    "Another useful way to review the distribution of each attribute is to use Box and Whisker Plots\n",
    "or boxplots for short. Boxplots summarize the distribution of each attribute, drawing a line for\n",
    "the median (middle value) and a box around the 25th and 75th percentiles (the middle 50% of\n",
    "the data). The whiskers give an idea of the spread of the data and dots outside of the whiskers\n",
    "show candidate outlier values (values that are 1.5 times greater than the size of spread of the\n",
    "middle 50% of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49344,
     "status": "ok",
     "timestamp": 1576086471481,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "0_aUqzB6XChT",
    "outputId": "8c2b8f5e-b706-4532-b7cb-ad359edc38cb"
   },
   "outputs": [],
   "source": [
    "# Box and Whisker Plots\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\n')\n",
    "print('Box and Whisker Plots:\\n ')\n",
    "s = data_to_use.plot(kind='box', color='red', subplots=True, layout=(3,3), sharex=False, sharey=False)\n",
    "\n",
    "# We can see that attributes do have quite dierent spreads. Given(if) the scales are the same, it\n",
    "# may suggest some benefit in standardizing the data for modelling to get all of the means lined\n",
    "# up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C95RCvX3XSOx"
   },
   "source": [
    "**********************************************************************************************\n",
    "### (2.B.2)Multivariate Plots\n",
    "**********************************************************************************************\n",
    "This section describes two plots that show the interactions between multiple variables\n",
    "in the dataset.\n",
    "\n",
    "1.   Correlation Matrix Plot.\n",
    "2.   Scatter Plot Matrix.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A2hp6C6mXTUQ"
   },
   "source": [
    "**Correlation Matrix Plot**\n",
    "\n",
    "Correlation gives an indication of how related the changes are between two variables. If two\n",
    "variables change in the same direction they are positively correlated. If they change in opposite\n",
    "directions together (one goes up, one goes down), then they are negatively correlated. We can\n",
    "calculate the correlation between each pair of attributes. This is called a correlation matrix. we\n",
    "can then plot the correlation matrix and get an idea of which variables have a high correlation with each other. This is useful to know, because some machine learning algorithms like linear and logistic regression can have poor performance if there are highly correlated input variables in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1576211744194,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "JyA2Fkc6XcXg",
    "outputId": "f12118e2-7d64-4270-bec5-dff107f5da2a"
   },
   "outputs": [],
   "source": [
    "# Correlation Matrix Plot\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\n')\n",
    "print('Correlation Matrix Plot:\\n ')\n",
    "correlations = data_to_use.corr()\n",
    "# plot correlation matrix\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,6,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "names = list(data_to_use.columns)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_yticklabels(names)\n",
    "plt.show()\n",
    "\n",
    "# The dark yellow color shows positive correlation whereas the dark mauve color shows negative\n",
    "# correlation. if there is strong correlatoin(+ve or -ve), this suggest candidates for removal\n",
    "# to better improve accuracy of models later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RH43Fy95BFxP"
   },
   "source": [
    "**********************************************************************************************\n",
    "# 3.Prepare the Data For Machine Learning\n",
    "**********************************************************************************************\n",
    "Many machine learning algorithms make assumptions about the data. It is often a very good\n",
    "idea to prepare the data in such way to best expose the structure of the problem to the machine\n",
    "learning algorithms that we intend to use. We will prepare\n",
    "the data for machine learning (Data Transforms) in Python using scikit-learn:\n",
    "\n",
    "\n",
    "1.   Discretization-Criteria\n",
    "2. Rescale data.\n",
    "3. Standardize data.\n",
    "4. Normalize data.\n",
    "5. Binarize data.\n",
    "6. Log Transform certain features\n",
    "7. Dealing with categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYNoJQ5oBKwz"
   },
   "source": [
    "**********************************************************************************************\n",
    "## A. Data Transforms\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gdrSIpeBPg1"
   },
   "source": [
    "Need For Data Pre-processing\n",
    "We almost always need to pre-process the data. It is a required step. A di\u000efficulty is that diff\u000b\n",
    "erent algorithms make diff\u000b\n",
    "erent assumptions about the data and may require diff\u000b\n",
    "erent transforms. Further, when we follow all of the rules and prepare the data, sometimes algorithms can deliver better results without pre-processing. Generally, We would recommend creating many diff\u000b\n",
    "erent views and transforms of the data, then exercise a handful of algorithms on each view of the dataset. This will help us to flush out which data transforms might be better at exposing the structure of our problem in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BbkJGPL_BbDy"
   },
   "source": [
    "**Data Transforms**\n",
    "\n",
    "We will work through 4 di\u000b\n",
    "fferent data pre-processing recipes for machine learning.\n",
    "\n",
    "Each recipe follows the same structure:\n",
    " \n",
    "1.   Load the dataset.\n",
    "2.   Split the dataset into the input and output variables for machine learning.\n",
    "3. Apply a pre-processing transform to the input variables.\n",
    "4. Summarize the data to show the change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qo6p7INoBkpv"
   },
   "source": [
    "The scikit-learn library provides two standard idioms for transforming data. Each are useful in diff\u000b\n",
    "erent circumstances. The transforms are calculated in such a way that they can be applied to the training data and any samples of data we may have in the future. The scikit-learn documentation has some information on how to use various di\u000b\n",
    "fferent pre-processing methods:\n",
    " Fit and Multiple Transform.\n",
    "Combined Fit-And-Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKkd1BFPCRDj"
   },
   "source": [
    "**********************************************************************************************\n",
    "### 3.A.2. Rescale Data\n",
    "**********************************************************************************************\n",
    "When the data is comprised of attributes with varying scales, many machine learning algorithms\n",
    "can benefi\f\n",
    "t from rescaling the attributes to all have the same scale. Often this is referred to\n",
    "as normalization and attributes are often rescaled into the range between 0 and 1. This is\n",
    "useful for optimization algorithms used in the core of machine learning algorithms like gradient\n",
    "descent. It is also useful for algorithms that weight inputs like regression and neural networks\n",
    "and algorithms that use distance measures like k-Nearest Neighbors. We can rescale the data\n",
    "using scikit-learn using the MinMaxScaler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1472,
     "status": "ok",
     "timestamp": 1576170266985,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "Hpe4gDfpB-RD",
    "outputId": "9fe5423c-95fd-42a8-ee9f-97afb34d40c6"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "no_of_coulmns = data_to_use.shape[1]\n",
    "no_of_attrib = no_of_coulmns -1\n",
    "\n",
    "array = data_to_use.values\n",
    "\n",
    "X = array[:,0:no_of_attrib]\n",
    "Y = array[:,no_of_attrib]\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "dataframe = pd.DataFrame.from_records(data=X_scale, \n",
    "                                      columns=data_to_use.drop(columns=['isDefault']).columns)\n",
    "\n",
    "dataframe['isDefault'] = Y\n",
    "\n",
    "data_to_use = dataframe\n",
    "data_to_use\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80lhCHvQBSbt"
   },
   "source": [
    "**********************************************************************************************\n",
    "## B. Feature Selection For Machine Learning\n",
    "**********************************************************************************************\n",
    "\n",
    "The data features that we use to train our machine learning models have a huge influence on the performance we can achieve. Irrelevant or partially relevant features can negatively impact model performance. We will discover automatic feature selection techniques\n",
    "that we can use to prepare our machine learning data in Python with scikit-learn. \n",
    "\n",
    "1. Univariate Selection.\n",
    "2. Recursive Feature Elimination.\n",
    "3. Principle Component Analysis.\n",
    "4. Feature Importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_qvq9mtBbCi"
   },
   "source": [
    "**********************************************************************************************\n",
    "### 3.B.5. Finding Important Features Using RandomForestClassifier\n",
    "**********************************************************************************************\n",
    "perform this task in the following steps:\n",
    "1. First, we need to create a random forests model.\n",
    "2. Second, use the feature importance variable to see feature importance scores.\n",
    "3. Third, visualize these scores using the seaborn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2486,
     "status": "ok",
     "timestamp": 1576210396557,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "4XUEOvVUBebR",
    "outputId": "906bfb86-de55-4967-ce75-b27c39542ef5"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "##############################################################################\n",
    "no_of_coulmns = data_to_use.shape[1]\n",
    "no_of_attrib = no_of_coulmns -1\n",
    "\n",
    "array = data_to_use.values\n",
    "\n",
    "X = array[:,0:no_of_attrib]\n",
    "Y = array[:,no_of_attrib]\n",
    "\n",
    "classifier_randomForest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Fitting the training data to the network\n",
    "classifier_randomForest.fit(X, Y)\n",
    "\n",
    "feature_imp = pd.Series(classifier_randomForest.feature_importances_,index=data_to_use.drop(columns=['isDefault']).columns).sort_values(ascending=False)\n",
    "\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\nFinding Important Features')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.legend(feature_imp)\n",
    "plt.show()\n",
    "\n",
    "print('\\nImportance of Features: \\n\\n',feature_imp)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1235,
     "status": "ok",
     "timestamp": 1576171775092,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "9fzgb5ngCA4W",
    "outputId": "9397ad5b-4872-4375-bc69-3bca5ba13e64"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "#Remove some features from Dataset\n",
    "list_to_delete=[]; list_to_delete.clear()\n",
    "\n",
    "# print('\\n\\n')\n",
    "for i in range(-2,0):\n",
    "  # print(i)\n",
    "  list_to_delete.append(feature_imp.index[i])\n",
    "\n",
    "# Delete multiple columns from the dataframe\n",
    "data_to_use = data_to_use.drop(list_to_delete, axis=1)\n",
    "data_to_use\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJ-_u11K1LGL"
   },
   "source": [
    "**********************************************************************************************\n",
    "# 5.Evaluate Some Algorithms\n",
    "**********************************************************************************************\n",
    "Now it is time to create some models of the data and estimate their accuracy on unseen data.\n",
    "Here is what we are going to cover in this step:\n",
    "1. Separate out a validation dataset.\n",
    "2. Setup the test harness to use 10-fold cross validation.\n",
    "3. Build 5 di\u000b\n",
    "erent models to predict species from \n",
    "ower measurements\n",
    "<!-- 4. Select the best model. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RvDzevW1MJu"
   },
   "source": [
    "**********************************************************************************************\n",
    "## 5.1 Create a Validation Dataset\n",
    "**********************************************************************************************\n",
    "We will split the loaded dataset into two, 67%\n",
    "of which we will use to train our models and 33% that we will hold back as a validation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1576210418853,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "PRNWTsUy1Qyv",
    "outputId": "6a52d10c-c4bd-48c3-c82d-4efc9edfc315"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "##############################################################################\n",
    "\n",
    "# Split-out validation dataset\n",
    "no_of_coulmns = data_to_use.shape[1]\n",
    "no_of_attrib = no_of_coulmns -1\n",
    "\n",
    "array = data_to_use.values\n",
    "# no_of_attributes = data_to_use_feature_selected.shape[1]\n",
    "# array = data_to_use_feature_selected.values\n",
    "# no_of_attributes = discretizedData.shape[1]\n",
    "# array = discretizedData.values\n",
    "\n",
    "X = array[0:10000,0:no_of_attrib]\n",
    "Y = array[0:10000,no_of_attrib]\n",
    "\n",
    "seed = 7\n",
    "\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=0.33,random_state=seed)\n",
    "X_validation, X_test, Y_validation, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5,random_state=seed)\n",
    "\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\nCreate a Training, Validation & Test Dataset:\\n')\n",
    "# Show the results of the split\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Validation set has {} samples.\".format(X_validation.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "\n",
    "# We now have training data in the X train and Y train for preparing models and a\n",
    "# X validation/test and Y validation/test sets that we can use later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWmukBSs1nmn"
   },
   "source": [
    "**********************************************************************************************\n",
    "## 5.2 Test Harness\n",
    "**********************************************************************************************\n",
    "We will use 10-fold cross validation to estimate accuracy. This will split our dataset into 10\n",
    "parts, train on 9 and test on 1 and repeat for all combinations of train-test splits. We are using\n",
    "the metric of accuracy to evaluate models. This is a ratio of the number of correctly predicted\n",
    "instances divided by the total number of instances in the dataset multiplied by 100 to give a\n",
    "percentage (e.g. 95% accurate). We will be using the scoring variable when we run build and\n",
    "evaluate each model next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbbrDsda1sMB"
   },
   "source": [
    "**********************************************************************************************\n",
    "## 5.3 Build Models\n",
    "**********************************************************************************************\n",
    "We don't know which algorithms would be good on this problem or what con\f\n",
    "figurations to use.\n",
    "We get an idea from the plots that some of the classes are partially linearly separable in some\n",
    "dimensions, so we are expecting generally good results. Let's evaluate six di\u000b\n",
    "erent algorithms:\n",
    "\n",
    "*   Logistic Regression (LR).\n",
    "*   Linear Discriminant Analysis (LDA).\n",
    "*   k-Nearest Neighbors (KNN).\n",
    "*   Classi\f\n",
    "cation and Regression Trees (CART).\n",
    "*   Gaussian Naive Bayes (NB).\n",
    "*   Support Vector Machines (SVM).\n",
    "\n",
    "This list is a good mixture of simple linear (LR and LDA), nonlinear (KNN, CART, NB\n",
    "and SVM) algorithms. We reset the random number seed before each run to ensure that the\n",
    "evaluation of each algorithm is performed using exactly the same data splits. It ensures the\n",
    "results are directly comparable. Let's build and evaluate our \f\n",
    "six models using **Spot-Check Classification Algorithms:**\n",
    "\n",
    "\n",
    "**What Techniques to Use When**\n",
    "\n",
    "This section lists some tips to consider what resampling technique to use in di\u000b\n",
    "fferent circum-stances.\n",
    "Generally k-fold cross validation is the gold standard for evaluating the performance of a machine learning algorithm on unseen data with k set to 3, 5, or 10.\n",
    "Using a train/test split is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets.\n",
    "Techniques like leave-one-out cross validation and repeated random splits can be useful intermediates when trying to balance variance in the estimated performance, model training speed and dataset size.\n",
    "The best advice is to experiment and \f\n",
    "find a technique for your problem that is fast and produces reasonable estimates of performance that you can use to make decisions. If in doubt, use 10-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rIuByvv1x-9"
   },
   "source": [
    "**********************************************************************************************\n",
    "### Spot-Check Classi\f\n",
    "cation Algorithms\n",
    "**********************************************************************************************\n",
    "Spot-checking is a way of discovering which algorithms perform well on our machine learning\n",
    "problem. We cannot know which algorithms are best suited to our problem beforehand. We\n",
    "must try a number of methods and focus attention on those that prove themselves the most\n",
    "promising. We will discover six machine learning algorithms that we can use\n",
    "when spot-checking our classi\f\n",
    "cation problem in Python with scikit-learn :\n",
    "1. How to spot-check machine learning algorithms on a classi\f\n",
    "cation problem.\n",
    "2. How to spot-check two linear classi\f\n",
    "cation algorithms.\n",
    "3. How to spot-check four nonlinear classi\f\n",
    "cation algorithms.\n",
    "\n",
    "We are going to take a look at six classi\f\n",
    "cation algorithms that we can spot-check on our\n",
    "dataset. Starting with **two linear machine learning algorithms**:\n",
    "\n",
    "1.   **Logistic Regression.**\n",
    "Logistic regression assumes a Gaussian distribution for the numeric input variables and can model binary classifi\f\n",
    "cation problems. we can construct a logistic regression model using the LogisticRegression class1.\n",
    "2.   **Linear Discriminant Analysis.** Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass classi\f\n",
    "fication. It too assumes a Gaussian distribution for the numerical input variables. We can construct an LDA model using the LinearDiscriminantAnalysis class\n",
    "\n",
    "Then looking at **four nonlinear machine learning algorithms**:\n",
    "\n",
    "*   **k-Nearest Neighbors.**The k-Nearest Neighbors algorithm (or KNN) uses a distance metric to \f\n",
    "find the k most similar instances in the training data for a new instance and takes the mean outcome of the neighbors as the prediction. You can construct a KNN model using the KNeighborsClassifier class\n",
    "*   **Naive Bayes.**Naive Bayes calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are estimated for new data and multiplied together,\n",
    "assuming that they are all independent (a simple or naive assumption). When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for input variables using the Gaussian Probability Density Function. We can construct a Naive\n",
    "Bayes model using the GaussianNB class\n",
    "*   **Classi\f\n",
    "cation and Regression Trees.**Classi\f\n",
    "cation and Regression Trees (CART or just decision trees) construct a binary tree from the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index).\n",
    "We can construct a CART model using the DecisionTreeClassifier class\n",
    "*   **Support Vector Machines.**Support Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors\n",
    "and influence where the line is placed. SVM has been extended to support multiple classes.\n",
    "Of particular importance is the use of di\u000b\n",
    "erent kernel functions via the kernel parameter. A powerful Radial Basis Function is used by default. We can construct an SVM model using the SVC class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23091,
     "status": "ok",
     "timestamp": 1576210690749,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "zsPgxk0B12Vy",
    "outputId": "9c77b0c0-9cbd-41a3-b3e3-f9026b6fa417"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "##############################################################################\n",
    "\n",
    "# Spot-Check Algorithms\n",
    "start=time.time()\n",
    "\n",
    "models = [] ; models.clear\n",
    "modelName = []; modelName.clear\n",
    "# Linear Regression\n",
    "# models.append(('Linear Regression', LinearRegression()))\n",
    "# Logistic Regression Classification\n",
    "models.append(('LR', LogisticRegression(C= 0.001, solver='lbfgs', multi_class='auto', max_iter=1000)))\n",
    "modelName.append('LR')\n",
    "# Linear Discriminant Analysis\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "modelName.append('LDA')\n",
    "# KNN Classification\n",
    "models.append(('KNN', KNeighborsClassifier(n_neighbors=7)))\n",
    "modelName.append('KNN')\n",
    "# Gaussian Naive Bayes Classification\n",
    "models.append(('NB', GaussianNB(priors=None, var_smoothing=1e-09)))\n",
    "modelName.append('NB')\n",
    "# Classication and Regression Trees (CART or just decision trees) Calssification\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "modelName.append('CART')\n",
    "# Support Vector Machines (or SVM) Calssification\n",
    "models.append(('SVM', SVC(gamma='auto', C=10, probability=True)))\n",
    "modelName.append('SVM')\n",
    "# XGBoost Calssification\n",
    "models.append(('XGBoost', XGBClassifier()))\n",
    "modelName.append('XGBoost')\n",
    "# MLPClassifier\n",
    "models.append(('MLP', MLPClassifier(activation='tanh', \n",
    "                                    alpha= 0.0001, hidden_layer_sizes= (100,), \n",
    "                                    learning_rate= 'adaptive', solver= 'lbfgs')))\n",
    "modelName.append('MLP')\n",
    "# Gradient Boosting Calssification\n",
    "models.append(('Gradient Boosting', GradientBoostingClassifier()))\n",
    "modelName.append('Gradient Boosting')\n",
    "# ExtraTreesClassifier\n",
    "models.append(('Extra Trees Classifier', ExtraTreesClassifier(n_estimators=100)))\n",
    "modelName.append('Extra Trees Classifier')\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "aucies = [] ; aucies.clear\n",
    "names = [] ; names.clear\n",
    "\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\nApplying Spot-Check Classication Algorithms:\\n')\n",
    "seed = 7\n",
    "for name, model in models:\n",
    "  # 10-fold cross validation: run the training algorithm 10 times, \n",
    "  # with a different 1/3 of the data as test set each time\n",
    "  kfold = KFold(n_splits=3, random_state=seed)\n",
    "  # print(model)\n",
    "  # cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy') \n",
    "  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='roc_auc') \n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  aucies.append(cv_results.mean())\n",
    "  msg = \"%s| auc_mean:%f auc_std(%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg) #mean estimated accuracy & Standard deviation\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLqSPaNW4oxp"
   },
   "source": [
    "**********************************************************************************************\n",
    "### Training Classifiers on the training data\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14071,
     "status": "ok",
     "timestamp": 1576210484005,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "SOZirqD_4pvS",
    "outputId": "0eaa71fd-77cb-4b96-b307-8b7483c0d6b4"
   },
   "outputs": [],
   "source": [
    "#  import and initialize the following classifers from sklearn:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "##############################################################################\n",
    "start=time.time()\n",
    "\n",
    "data_ = [] ; data_.clear()\n",
    "\n",
    "#for HCV-Egy-Data\n",
    "data_.append(('Bank Customer Transactions Dataset:', X_train, Y_train ))\n",
    "# We now have training data in the X train and Y train for preparing models.\n",
    "\n",
    "accuracies = [] ; accuracies.clear\n",
    "names = [] ; names.clear\n",
    "matrix_accuracy = np.zeros((1,len(models)))\n",
    "\n",
    "i=0;\n",
    "\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\nTraining Classifiers on the training data:\\n')\n",
    "\n",
    "for dataname, Xtrain, Ytrain in data_:\n",
    "  print(dataname)\n",
    "  j = 0\n",
    "  for name, model in models:\n",
    "\n",
    "    #Fitting the training data to the network\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Using the trained network to predict\n",
    "    accuracy = model.score(X_validation, Y_validation)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    names.append(name)\n",
    "    msg = \"%s (accuracy): %f\" % (name, accuracy)\n",
    "    print(msg) # name of classifier & its accuracy after training it.\n",
    "    matrix_accuracy[i][j] = accuracy\n",
    "    j = j + 1\n",
    "  i = i + 1\n",
    "  print('\\n')\n",
    "\n",
    "print('Matrix for Accuracy-Rate (Dataset vs. Classifiers) :\\n\\n' , matrix_accuracy)\n",
    "\n",
    "accuracies.index(max(accuracies))\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAAzuI_b9Yhj"
   },
   "source": [
    "### Ensemble Methods\n",
    "\n",
    "Another way that we can improve the performance of algorithms on this problem is by using\n",
    "ensemble methods. In this section we will evaluate four different ensemble machine learning\n",
    "algorithms, two boosting and two bagging methods:\n",
    "\n",
    "*   Boosting Methods: AdaBoost (AB) and Gradient Boosting (GBM).\n",
    "*   Bagging Methods: Random Forests (RF) and Extra Trees (ET).\n",
    "\n",
    "We will use the same test harness as before, 10-fold cross validation. No data standardization is used in this case because all four ensemble algorithms are based on decision trees that are\n",
    "less sensitive to data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45231,
     "status": "ok",
     "timestamp": 1576124212215,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "5TyxPWVN9fGn",
    "outputId": "605b0841-84e6-4bbc-f3c2-387f9c4f6831"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "##############################################################################\n",
    "\n",
    "start=time.time()\n",
    "max_features_ = data_to_use.shape[1]-1\n",
    "# ensembles\n",
    "ensembles = []\n",
    "ensembles.append(('AB', AdaBoostClassifier()))\n",
    "ensembles.append(('GBM', GradientBoostingClassifier()))\n",
    "ensembles.append(('RF', RandomForestClassifier(n_estimators=100, max_features=max_features_)))\n",
    "ensembles.append(('ET', ExtraTreesClassifier()))\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('\\nApplying Ensemble Methods to improve the performance of algorithms:\\n')\n",
    "\n",
    "num_folds = 10\n",
    "scoring = 'roc_auc'\n",
    "for name, model in ensembles:\n",
    "  kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s| auc_mean:%f auc_std(%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "  # '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIFsDp7a96CY"
   },
   "source": [
    "**********************************************************************************************\n",
    "### Plotting Accuracy Matrix (Dataset vs. Classifiers)\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 540,
     "status": "error",
     "timestamp": 1576251923691,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "pQ8z4t0B965M",
    "outputId": "c9478f92-64e8-4595-f569-b12ba033971f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "##############################################################################\n",
    "\n",
    "# Limits for the extent\n",
    "size = len(models)\n",
    "x_start = 3.0\n",
    "x_end = 9.0\n",
    "y_start = 6.0\n",
    "y_end = 7.0\n",
    "\n",
    "extent = [x_start, x_end, y_start, y_end]\n",
    "\n",
    "# The normal figure\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "im = ax.imshow(matrix_accuracy, extent=extent, origin='lower', interpolation='None', cmap='viridis')\n",
    "\n",
    "# Add the text\n",
    "jump_x = (x_end - x_start) / (2.0 * size)\n",
    "jump_y = (y_end - y_start) / (2.0 * size)\n",
    "x_positions = np.linspace(start=x_start, stop=x_end, num=size, endpoint=False)\n",
    "y_positions = np.linspace(start=y_start, stop=y_end, num=1, endpoint=False)\n",
    "\n",
    "for y_index, y in enumerate(y_positions):\n",
    "    for x_index, x in enumerate(x_positions):\n",
    "        label = format(matrix_accuracy[y_index, x_index],'.6f')\n",
    "        text_x = x + jump_x\n",
    "        text_y = y + jump_y\n",
    "        ax.text(text_x, text_y, label,  ha='center', va='center')\n",
    "\n",
    "fig.colorbar(im)\n",
    "\n",
    "ax.tick_params(direction='out', length=6, width=2, colors='r',\n",
    "               grid_color='r', grid_alpha=0.5)\n",
    "ax.set_title('Accuracy-Rate: \\n Training each classifier on a dataset', \n",
    "             fontsize='large', fontweight='bold')\n",
    "ax.set_xlabel(modelName, fontsize='large', fontweight='bold')\n",
    "ax.set_ylabel('Bank Customer Transactions Dataset', fontsize='large', fontweight='bold')\n",
    "\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('Plotting Accuracy Matrix (Dataset vs. Classifiers)\\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVxT1Icr_SgA"
   },
   "source": [
    "## 5.4 Evaluate Algorithms: Standardize Data\n",
    "\n",
    "We suspect that the differing distributions of the raw data may be negatively impacting the skill of some of the algorithms. Let's evaluate the same algorithms with a standardized copy of the dataset. This is where the data is transformed such that each attribute has a mean value of zero and a standard deviation of one. We also need to avoid data leakage when we transform the data. A good way to avoid leakage is to use pipelines that standardize the data and build the model for each fold in the cross validation test harness. That way we can get a fair estimation of how each model with standardized data might perform on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 114757,
     "status": "ok",
     "timestamp": 1576170583648,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "peYQQnJW_Tcv",
    "outputId": "4a75eebc-9651-43b2-8a4c-e4fe73fafaa6"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "##############################################################################\n",
    "\n",
    "# Standardize the dataset\n",
    "\n",
    "start=time.time()\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',\n",
    "LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000))])))\n",
    "pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA',\n",
    "LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',\n",
    "KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB',\n",
    "GaussianNB())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART',\n",
    "DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC(gamma='auto'))])))\n",
    "pipelines.append(('ScaledXGBoost', Pipeline([('Scaler', StandardScaler()),('XGBoost', XGBClassifier())])))\n",
    "pipelines.append(('ScaledMLP', Pipeline([('Scaler', StandardScaler()),('MLP', MLPClassifier(activation='tanh', \n",
    "                                    alpha= 0.0001, hidden_layer_sizes= (100,), \n",
    "                                    learning_rate= 'adaptive', solver= 'lbfgs'))])))\n",
    "pipelines.append(('ScaledGB', Pipeline([('Scaler', StandardScaler()),('Gradient Boosting',\n",
    "GradientBoostingClassifier())])))\n",
    "pipelines.append(('ScaledExtra Tree Classiffier', Pipeline([('Scaler', StandardScaler()),('Extra Tree Classiffier',\n",
    "ExtraTreesClassifier(n_estimators=100))])))\n",
    "\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('Evaluate Algorithms: Standardize Data\\n')\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'roc_auc'\n",
    "for name, model in pipelines:\n",
    "  kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s| auc_mean:%f auc_std(%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "  # '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9p0_o1z8CbQ"
   },
   "source": [
    "**********************************************************************************************\n",
    "# Tuning XGBoost with Grid Search CV\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYe-qB0ORPnf"
   },
   "outputs": [],
   "source": [
    "seed = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lejcHW-wix6F"
   },
   "source": [
    "**********************************************************************************************\n",
    "## Step 1- Find the number of estimators for a high learning rate\n",
    "*********************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29138,
     "status": "ok",
     "timestamp": 1576200335605,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "Ix6qSn-J8Fdh",
    "outputId": "f8cbbe9e-3a02-4e3f-d229-a1c450a20e5b"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "\n",
    "# Tune the classiffier\n",
    "start=time.time()\n",
    "\n",
    "# tunning Parameters:\n",
    "parameter_space = {\n",
    "     'n_estimators' : np.arange(100, 1000, 100) #Start, Stop, Increment\n",
    "}\n",
    "\n",
    "#Start with default parameters\n",
    "model = XGBClassifier()\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "scoring = 'roc_auc'\n",
    "grid = GridSearchCV(estimator=model, param_grid=parameter_space, scoring=scoring, n_jobs=4, cv=kfold)\n",
    "\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "print(\"Best AUC : %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"auc_mean:%f| auc_std:%f param(%s)\" % (mean, stdev, param))\n",
    "\n",
    "print('\\n')\n",
    "print(grid_result.best_estimator_)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdFr3kipk_LV"
   },
   "source": [
    "**********************************************************************************************\n",
    "## Step 2: Tune max_depth and min_child_weight\n",
    "*********************************************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30219,
     "status": "ok",
     "timestamp": 1576200408944,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "Hso6rwBxlIm4",
    "outputId": "0e899de1-71e5-488a-d94f-b826273b5ba3"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "\n",
    "# Tune the classiffier\n",
    "start=time.time()\n",
    "\n",
    "# tunning Parameters:\n",
    "parameter_space = {\n",
    "    'max_depth':range(3,10,2),\n",
    "    'min_child_weight':range(1,6,2),\n",
    "}\n",
    "\n",
    "model = XGBClassifier(n_estimators = 100)\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "scoring = 'roc_auc'\n",
    "grid = GridSearchCV(estimator=model, param_grid=parameter_space, scoring=scoring, n_jobs=4, cv=kfold)\n",
    "\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "print(\"Best AUC : %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"auc_mean:%f| auc_std:%f param(%s)\" % (mean, stdev, param))\n",
    "\n",
    "print('\\n')\n",
    "print(grid_result.best_estimator_)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L03315JJm9cz"
   },
   "source": [
    "Lets go one step deeper and look for optimum values. We’ll search for values 1 above and below the optimum values because we took an interval of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14345,
     "status": "ok",
     "timestamp": 1576200563277,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "--cNiVGJm-kQ",
    "outputId": "5adf4d6c-d561-4916-c8a8-4486592eaa5a"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "\n",
    "# Tune the classiffier\n",
    "start=time.time()\n",
    "\n",
    "# tunning Parameters:\n",
    "parameter_space = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'min_child_weight':[4,5,6]\n",
    "}\n",
    "\n",
    "#Start with default parameters\n",
    "model = XGBClassifier(n_estimators = 100)\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "scoring = 'roc_auc'\n",
    "grid = GridSearchCV(estimator=model, param_grid=parameter_space, scoring=scoring, n_jobs=4, cv=kfold)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "print(\"Best AUC : %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"auc_mean:%f| auc_std:%f param(%s)\" % (mean, stdev, param))\n",
    "\n",
    "print('\\n')\n",
    "print(grid_result.best_estimator_)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixNT0vVWpCgf"
   },
   "source": [
    "**********************************************************************************************\n",
    "## Step 3: Tune gamma\n",
    "*********************************************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8382,
     "status": "ok",
     "timestamp": 1576200628605,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "3tJE4XylpE7B",
    "outputId": "17a196f1-95d5-4f2e-8da7-6531841de4eb"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "\n",
    "# Tune the classiffier\n",
    "start=time.time()\n",
    "\n",
    "# tunning Parameters:\n",
    "parameter_space = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "#Start with default parameters\n",
    "model = XGBClassifier(n_estimators = 100, max_depth = 2, min_child_weight = 5)\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "scoring = 'roc_auc'\n",
    "grid = GridSearchCV(estimator=model, param_grid=parameter_space, scoring=scoring, n_jobs=4, cv=kfold)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "print(\"Best AUC : %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"auc_mean:%f| auc_std:%f param(%s)\" % (mean, stdev, param))\n",
    "\n",
    "print('\\n')\n",
    "print(grid_result.best_estimator_)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfZ_Cl9ZpoaN"
   },
   "source": [
    "**********************************************************************************************\n",
    "## Step 4: Tune subsample and colsample_bytree\n",
    "*********************************************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15089,
     "status": "ok",
     "timestamp": 1576200703117,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "T_rMeWpOpr5i",
    "outputId": "f17f0fde-71e6-46e1-b6b1-36a925bac8ce"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "\n",
    "# Tune the classiffier\n",
    "start=time.time()\n",
    "\n",
    "# tunning Parameters:\n",
    "parameter_space = {\n",
    "    'subsample':[i/10.0 for i in range(6,10)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "\n",
    "#Start with default parameters\n",
    "model = XGBClassifier(n_estimators = 100, max_depth = 2, min_child_weight = 5,\n",
    "                      gamma = 0)\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "scoring = 'roc_auc'\n",
    "grid = GridSearchCV(estimator=model, param_grid=parameter_space, scoring=scoring, n_jobs=4, cv=kfold)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "print(\"Best AUC : %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"auc_mean:%f| auc_std:%f param(%s)\" % (mean, stdev, param))\n",
    "\n",
    "print('\\n')\n",
    "print(grid_result.best_estimator_)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fA7hKmkaqIwR"
   },
   "source": [
    "Here, we found 0.7 & 0.9 as the optimum value for  subsample and colsample_bytree respectively. \n",
    "But the values tried are very widespread, we should try values closer to the optimum values here (0.7 & 0.9) to see if we get something better.\n",
    "\n",
    "Now we should try values in 0.05 interval around these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20616,
     "status": "ok",
     "timestamp": 1576201229326,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "2pN2F1D3qXIq",
    "outputId": "1ea95553-94c3-4076-f19d-17e40b2c5262"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "\n",
    "# Tune the classiffier\n",
    "start=time.time()\n",
    "\n",
    "# tunning Parameters:\n",
    "parameter_space = {\n",
    "    'subsample':[i/100.0 for i in range(65,80,4)],\n",
    "    'colsample_bytree':[i/100.0 for i in range(85,100,4)]\n",
    "}\n",
    "\n",
    "#Start with default parameters\n",
    "model = XGBClassifier(n_estimators = 100, max_depth = 3, min_child_weight = 4,\n",
    "                      gamma = 0,\n",
    "                      subsample = 0.7, colsample_bytree = 0.9)\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "scoring = 'roc_auc'\n",
    "grid = GridSearchCV(estimator=model, param_grid=parameter_space, scoring=scoring, n_jobs=4, cv=kfold)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "print(\"Best AUC : %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"auc_mean:%f| auc_std:%f param(%s)\" % (mean, stdev, param))\n",
    "\n",
    "print('\\n')\n",
    "print(grid_result.best_estimator_)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CX0rWYABrRPB"
   },
   "source": [
    "**********************************************************************************************\n",
    "## Step 5: Tuning Regularization Parameters\n",
    "*********************************************************************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZEB_SZ7rcuV"
   },
   "source": [
    "Next step is to apply regularization to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10396,
     "status": "ok",
     "timestamp": 1576203759733,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "0h8WSjZwrYOL",
    "outputId": "e7daebd6-3a35-4e80-9041-7bc41b3bba26"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "\n",
    "# Tune the classiffier\n",
    "start=time.time()\n",
    "\n",
    "# tunning Parameters:\n",
    "parameter_space = {\n",
    "    'reg_alpha':[0, 1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "#Start with default parameters\n",
    "# model = XGBClassifier(n_estimators = 100, max_depth = 3, min_child_weight = 4,\n",
    "#                       gamma = 0,\n",
    "#                       subsample = 0.7, colsample_bytree = 0.9)\n",
    "\n",
    "model = XGBClassifier(n_estimators = 100, max_depth = 3, min_child_weight = 4,\n",
    "                      gamma = 0)\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "scoring = 'roc_auc'\n",
    "grid = GridSearchCV(estimator=model, param_grid=parameter_space, scoring=scoring, n_jobs=4, cv=kfold)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "print(\"Best AUC : %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"auc_mean:%f| auc_std:%f param(%s)\" % (mean, stdev, param))\n",
    "\n",
    "print('\\n')\n",
    "print(grid_result.best_estimator_)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "# '''\n",
    "\n",
    "\n",
    "# #####################\n",
    "# # ''' Remove this comment out if needs to be used\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# # Randomized for Algorithm Tuning\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# ##############################################################################\n",
    "\n",
    "# maxFeatures = range(1,data_to_use.shape[1]-1)\n",
    "# param_grid = {'reg_alpha': uniform()}\n",
    "# model = XGBClassifier(n_estimators = 100, max_depth = 3, min_child_weight = 4,\n",
    "#                       gamma = 0,\n",
    "#                       subsample = 0.7, colsample_bytree = 0.9)\n",
    "# scoring = 'roc_auc'\n",
    "# rsearch =RandomizedSearchCV(estimator=model, param_distributions=param_grid\n",
    "#                             , cv=5, scoring=scoring, n_iter=len(maxFeatures), random_state=seed)\n",
    "# rsearch.fit(X, Y)\n",
    "# print(\"Best accuracy is \"+ str(rsearch.best_score_))\n",
    "# print(rsearch.best_estimator_)\n",
    "# # '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CAflC4O7MK-"
   },
   "source": [
    "We can see that the AUC score is less than the previous case. But the values tried are very widespread, we should try values closer to the optimum here (0.1) to see if we get something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9581,
     "status": "ok",
     "timestamp": 1576201938229,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "4WRSw8567Se4",
    "outputId": "af581b72-3bca-44b7-f2d0-e7552f028806"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "\n",
    "# Tune the classiffier\n",
    "start=time.time()\n",
    "\n",
    "# tunning Parameters:\n",
    "parameter_space = {\n",
    "    'reg_alpha':[0, 0.01, 0.05, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "#Start with default parameters\n",
    "model = XGBClassifier(n_estimators = 100, max_depth = 3, min_child_weight = 4,\n",
    "                      gamma = 0,\n",
    "                      subsample = 0.7, colsample_bytree = 0.9)\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "scoring = 'roc_auc'\n",
    "grid = GridSearchCV(estimator=model, param_grid=parameter_space, scoring=scoring, n_jobs=4, cv=kfold)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "print(\"Best AUC : %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"auc_mean:%f| auc_std:%f param(%s)\" % (mean, stdev, param))\n",
    "\n",
    "print('\\n')\n",
    "print(grid_result.best_estimator_)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QcEwO1voCiuR"
   },
   "source": [
    "**********************************************************************************************\n",
    "## Step 6: Reducing Learning Rate\n",
    "*********************************************************************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9LbMse9C3mb"
   },
   "source": [
    "Lastly, we should lower the learning rate and add more trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9284,
     "status": "ok",
     "timestamp": 1576204512676,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "UBYKtXfDCnEz",
    "outputId": "8cadef86-33ab-4dd0-bfd6-bba81e26c1f4"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "\n",
    "# Tune the classiffier\n",
    "start=time.time()\n",
    "\n",
    "# tunning Parameters:\n",
    "parameter_space = {\n",
    "    'learning_rate':[0.01, 0.1]\n",
    "}\n",
    "\n",
    "#Start with default parameters\n",
    "# model = XGBClassifier(n_estimators = 100, max_depth = 3, min_child_weight = 4,\n",
    "#                       gamma = 0,\n",
    "#                       subsample = 0.7, colsample_bytree = 0.9)\n",
    "\n",
    "model = XGBClassifier(n_estimators = 100, max_depth = 3, min_child_weight = 4,\n",
    "                      gamma = 0)\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "scoring = 'roc_auc'\n",
    "grid = GridSearchCV(estimator=model, param_grid=parameter_space, scoring=scoring, n_jobs=4, cv=kfold)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "print(\"Best AUC : %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"auc_mean:%f| auc_std:%f param(%s)\" % (mean, stdev, param))\n",
    "\n",
    "print('\\n')\n",
    "print(grid_result.best_estimator_)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "# '''\n",
    "\n",
    "\n",
    "#####################\n",
    "# ''' Remove this comment out if needs to be used\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Randomized for Algorithm Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "##############################################################################\n",
    "\n",
    "maxFeatures = range(1,data_to_use.shape[1]-1)\n",
    "param_grid = {'learning_rate': uniform()}\n",
    "model = XGBClassifier(n_estimators = 100, max_depth = 3, min_child_weight = 4,\n",
    "                      gamma = 0,\n",
    "                      subsample = 0.7, colsample_bytree = 0.9)\n",
    "scoring = 'roc_auc'\n",
    "rsearch =RandomizedSearchCV(estimator=model, param_distributions=param_grid\n",
    "                            , cv=5, scoring=scoring, n_iter=len(maxFeatures), random_state=seed)\n",
    "rsearch.fit(X, Y)\n",
    "print(\"Best accuracy is \"+ str(rsearch.best_score_))\n",
    "print(rsearch.best_estimator_)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1207,
     "status": "ok",
     "timestamp": 1576204532071,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "IjIZtTFQQEIB",
    "outputId": "e7472e5d-f5a8-4bfc-8f4f-d851d302d9ed"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used \n",
    "##############################################################################\n",
    "xGradientBoosting = grid_result.best_estimator_\n",
    "xGradientBoosting.fit(X_train,Y_train)\n",
    "xgbPredict = xGradientBoosting.predict(X_validation)\n",
    "xgbPredictproba = xGradientBoosting.predict_proba(X_validation)[:,1] #for ROC curve\n",
    "xgbAccuracy = accuracy_score(Y_validation,xgbPredict)\n",
    "roc_score = metrics.roc_auc_score(Y_validation,xgbPredict)\n",
    "print(\"(VALIDATION DATA) X-Gradient Boosting accuracy is \",xgbAccuracy)\n",
    "plotRocCurve(xgbPredictproba, Y_validation, 'X-Gradient Boosting')\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1185,
     "status": "ok",
     "timestamp": 1576204539377,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "9QH3GkJ4iVAa",
    "outputId": "3af3289e-5c1f-4ba8-859b-792f83a6cde8"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used \n",
    "##############################################################################\n",
    "xGradientBoosting = grid_result.best_estimator_\n",
    "xGradientBoosting.fit(X_train,Y_train)\n",
    "xgbPredict = xGradientBoosting.predict(X_test)\n",
    "xgbPredictproba = xGradientBoosting.predict_proba(X_test)[:,1] #for ROC curve\n",
    "xgbAccuracy = accuracy_score(Y_test,xgbPredict)\n",
    "roc_score = metrics.roc_auc_score(Y_test,xgbPredict)\n",
    "print(\"(TEST DATA) X-Gradient Boosting accuracy is \",xgbAccuracy)\n",
    "plotRocCurve(xgbPredictproba, Y_test, 'X-Gradient Boosting')\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1223,
     "status": "ok",
     "timestamp": 1576204545457,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "DHFZeCqmBP0u",
    "outputId": "2f78aa8d-eaef-4f34-ab57-57cb502bfeb6"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used \n",
    "##############################################################################\n",
    "xGradientBoosting = grid_result.best_estimator_\n",
    "xGradientBoosting.fit(X_train,Y_train)\n",
    "xgbPredict = xGradientBoosting.predict(X_train)\n",
    "xgbPredictproba = xGradientBoosting.predict_proba(X_train)[:,1] #for ROC curve\n",
    "xgbAccuracy = accuracy_score(Y_train,xgbPredict)\n",
    "roc_score = metrics.roc_auc_score(Y_train,xgbPredict)\n",
    "print(\"(TRAINING DATA) X-Gradient Boosting accuracy is \",xgbAccuracy)\n",
    "plotRocCurve(xgbPredictproba, Y_train, 'X-Gradient Boosting')\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjIGdAq4aJSR"
   },
   "source": [
    "**********************************************************************************************\n",
    "## Tuning XGBoost with Randomized Search\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23579,
     "status": "ok",
     "timestamp": 1576178910741,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "s1CKff8HabXP",
    "outputId": "e21145ab-8487-46f8-b288-0f7f52deb437"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "# Randomized for Algorithm Tuning\n",
    "##############################################################################\n",
    "start=time.time()\n",
    "\n",
    "no_of_coulmns = data_to_use.shape[1]\n",
    "no_of_attrib = no_of_coulmns -1\n",
    "\n",
    "array = data_to_use.values\n",
    "X = array[:,0:no_of_attrib]\n",
    "Y = array[:,no_of_attrib]\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "\n",
    "maxFeatures = range(1,data_to_use.shape[1]-1)\n",
    "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "max_depth_list = [i for i in range(1,11)]\n",
    "estimators_list = [10**i for i in range(1,3)]\n",
    "# param_grid = {'learning_rate': uniform(0,1), 'n_estimators': uniform(100,500) }\n",
    "# param_grid = dict(max_features=maxFeatures,learning_rate= lr_list, \n",
    "#                   max_depth = max_depth_list, n_estimators = estimators_list )\n",
    "param_grid = {'learning_rate': uniform()}\n",
    "# model = XGBClassifier(n_estimators = 100, max_depth = 10)\n",
    "#INITIALY\n",
    "model = XGBClassifier(n_estimators = 100,  max_depth=5,\n",
    "                      min_child_weight = 1, gamma = 0, subsample = 0.8,\n",
    "                      colsample_bytree = 0.8, scale_pos_weight=1)\n",
    "\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
    "xgbSearch =RandomizedSearchCV(estimator=model, param_distributions=param_grid\n",
    "                            , cv=10, scoring='roc_auc', n_iter=len(maxFeatures), random_state=seed)\n",
    "xgbSearch.fit(X_train, Y_train)\n",
    "print(\"Best auc is \"+ str(xgbSearch.best_score_))\n",
    "print(xgbSearch.best_estimator_)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2711,
     "status": "ok",
     "timestamp": 1576178052012,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "254pnBI3bRZE",
    "outputId": "39b5c9ed-3bac-45c6-ce4c-b8adefd655c1"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used \n",
    "##############################################################################\n",
    "xGradientBoosting = xgbSearch.best_estimator_\n",
    "xGradientBoosting.fit(X_train,Y_train)\n",
    "xgbPredict = xGradientBoosting.predict(X_validation)\n",
    "xgbPredictproba = xGradientBoosting.predict_proba(X_validation)[:,1] #for ROC curve\n",
    "xgbAccuracy = accuracy_score(Y_validation,xgbPredict)\n",
    "roc_score = metrics.roc_auc_score(Y_validation,xgbPredict)\n",
    "print(\"X-Gradient Boosting accuracy is \",xgbAccuracy)\n",
    "plotRocCurve(xgbPredictproba, Y_validation, 'X-Gradient Boosting')\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NnjOb2euqSBX"
   },
   "source": [
    "**********************************************************************************************\n",
    "## Tuning SVM with Grid Search CV\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 108591,
     "status": "ok",
     "timestamp": 1576086531246,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "d_0u_1h4qczk",
    "outputId": "80aba8e5-0d88-4fbb-d74a-71d84f31924e"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "##############################################################################\n",
    "\n",
    "# Tune scaled SVM\n",
    "start=time.time(\n",
    "    \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "# tunning Parameters:\n",
    "parameter_space = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "model = SVC(gamma='auto', probability=True) \n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "scoring = 'accuracy'\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid = GridSearchCV(model, parameter_space, n_jobs=-1, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print(model)\n",
    "# model.get_params().keys()\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5E00RnSO1CV"
   },
   "source": [
    "## Tuning Support Vector Machines(SVM) with Grid Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7xYQokePFKb"
   },
   "source": [
    "SVM (Support Vector Machines) when implemented with grid search, we got the best accuracies and minimum false negatives. We used the Grid search to find the best hyper paramters for the model.Later we used this value to find the predictions and plot the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 285954,
     "status": "ok",
     "timestamp": 1576104702636,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "8HSvyMLtPGqC",
    "outputId": "eb394df6-71dc-484e-f8e6-8e281ce857af"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "##############################################################################\n",
    "start=time.time()\n",
    "\n",
    "no_of_coulmns = data_to_use.shape[1]\n",
    "no_of_attrib = no_of_coulmns -1\n",
    "\n",
    "array = data_to_use.values\n",
    "X = array[:,0:no_of_attrib]\n",
    "Y = array[:,no_of_attrib]\n",
    "\n",
    "powers = range(1,2)\n",
    "# powers = range(-2,5)\n",
    "cs = [10**i for i in powers]\n",
    "param_grid = dict(C=cs)\n",
    "\n",
    "# tunning Parameters:\n",
    "parameter_space = param_grid\n",
    "\n",
    "model = SVC(gamma='auto', probability=True) \n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "scoring = 'roc_auc'#'accuracy'\n",
    "grid = GridSearchCV(estimator=model, param_grid=parameter_space, scoring=scoring, cv=kfold)\n",
    "\n",
    "grid_result = grid.fit(X, Y)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print(model)\n",
    "# model.get_params().keys()\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fp7Ml_RpXTVx"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "clf_svm = model\n",
    "clf_svm.fit(X_train,Y_train)\n",
    "predictions_svm = clf_svm.predict(X_test)\n",
    "predictproba_svm = clf_svm.decision_function(X_test)\n",
    "SVM_Accuracy = accuracy_score(Y_test,predictions_svm)\n",
    "print(\"SVM accuracy is \",SVM_Accuracy)\n",
    "plotAUC(Y_test,predictproba_svm, 'SVM')\n",
    "# plotAUC(Y_test,rfPredictproba, 'Random Forest')\n",
    "# plotAUC(y_test,LR_Predict,'Logistic Regression')\n",
    "plt.show()\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(predictions_svm, normalize=True)\n",
    "plt.show()\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e_qPkWEhbRM"
   },
   "source": [
    "## Random Forest with randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "deZvtETMho6L"
   },
   "source": [
    "Random forest when implemented with randomized search we got the best accuracies and minimum false negatives(predicting borowwer will not default eventhough he will. This might impact on the credibility of the company). We used the randomized search to find the best hyper paramters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTHQOxZIhqII"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Randomized for Algorithm Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "##############################################################################\n",
    "no_of_coulmns = data_to_use.shape[1]\n",
    "no_of_attrib = no_of_coulmns -1\n",
    "\n",
    "array = data_to_use.values\n",
    "X = array[:,0:no_of_attrib]\n",
    "Y = array[:,no_of_attrib]\n",
    "\n",
    "maxFeatures = range(1,data_to_use.shape[1]-1)\n",
    "param_grid = dict(max_features=maxFeatures)\n",
    "# param_grid = {'alpha': uniform()}\n",
    "model = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
    "rsearch =RandomizedSearchCV(estimator=model, param_distributions=param_grid\n",
    "                            , cv=10, scoring='accuracy', n_iter=len(maxFeatures), random_state=seed)\n",
    "rsearch.fit(X, Y)\n",
    "print(\"Best accuracy is \"+ str(rsearch.best_score_))\n",
    "print(rsearch.best_estimator_)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsxZk2Apra2u"
   },
   "outputs": [],
   "source": [
    "''' Remove this comment out if needs to be used \n",
    "##############################################################################\n",
    "\n",
    "randomForest = rsearch.best_estimator_\n",
    "randomForest.fit(X_train,Y_train)\n",
    "rfPredict = randomForest.predict(X_test)\n",
    "rfPredictproba = randomForest.predict_proba(X_test)[:,1] #for ROC curve\n",
    "rfAccuracy = accuracy_score(Y_test,rfPredict)\n",
    "roc_score = metrics.roc_auc_score(Y_test,rfPredict)\n",
    "print(\"Random Forest accuracy is \",rfAccuracy)\n",
    "plotRocCurve(rfPredictproba, Y_test, 'Random Forest')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EJXZbi0jOV6m"
   },
   "source": [
    "##Tuning Gradient Boosting with randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_2mvxCxPPO_"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used\n",
    "# Randomized for Algorithm Tuning\n",
    "##############################################################################\n",
    "start=time.time()\n",
    "\n",
    "no_of_coulmns = data_to_use.shape[1]\n",
    "no_of_attrib = no_of_coulmns -1\n",
    "\n",
    "array = data_to_use.values\n",
    "X = array[:,0:no_of_attrib]\n",
    "Y = array[:,no_of_attrib]\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "\n",
    "maxFeatures = range(1,data_to_use.shape[1]-1)\n",
    "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "max_depth_list = [i for i in range(1,11)]\n",
    "estimators_list = [10**i for i in range(1,5)]\n",
    "# param_grid = {'learning_rate': uniform(), 'n_estimators': estimators_list }\n",
    "param_grid = dict(max_features=maxFeatures,learning_rate= lr_list, \n",
    "                  max_depth = max_depth_list, n_estimators = estimators_list )\n",
    "# param_grid = {'alpha': uniform()}\n",
    "model = GradientBoostingClassifier()\n",
    "# model = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
    "gbSearch =RandomizedSearchCV(estimator=model, param_distributions=param_grid\n",
    "                            , cv=10, scoring='roc_auc', n_iter=len(maxFeatures), random_state=seed)\n",
    "gbSearch.fit(X, Y)\n",
    "print(\"Best auc is \"+ str(gbSearch.best_score_))\n",
    "print(gbSearch.best_estimator_)\n",
    "\n",
    "end=time.time()\n",
    "print(\"\\nThis script took {} minutes to complete\".format(round((end-start)/60,2)))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1654,
     "status": "ok",
     "timestamp": 1576127066042,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "https://lh6.googleusercontent.com/-CB1InUHULDk/AAAAAAAAAAI/AAAAAAAAAk8/YwW1FWoaEio/s64/photo.jpg",
      "userId": "06196583618356137904"
     },
     "user_tz": 300
    },
    "id": "a2MCWKZ2Wzeb",
    "outputId": "09c8baa9-e602-412f-f013-eb7e7c6b57ab"
   },
   "outputs": [],
   "source": [
    "# ''' Remove this comment out if needs to be used \n",
    "##############################################################################\n",
    "gradientBoosting = gbSearch.best_estimator_\n",
    "gradientBoosting.fit(X_train,Y_train)\n",
    "gbPredict = gradientBoosting.predict(X_test)\n",
    "gbPredictproba = gradientBoosting.predict_proba(X_test)[:,1] #for ROC curve\n",
    "gbAccuracy = accuracy_score(Y_test,gbPredict)\n",
    "roc_score = metrics.roc_auc_score(Y_test,gbPredict)\n",
    "print(\"Gradient Boosting accuracy is \",gbAccuracy)\n",
    "plotRocCurve(gbPredictproba, Y_test, 'Gradient Boosting')\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75v-BXosvkE9"
   },
   "outputs": [],
   "source": [
    "''' Remove this comment out if needs to be used\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(rfPredict, normalize=True)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ibXBxmvp-LYc"
   },
   "source": [
    "**********************************************************************************************\n",
    "# Select Best Model\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1576210777756,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "CX-4JEk8-Mh-",
    "outputId": "8e538773-4226-4e25-cd16-0b9873b1ea7a"
   },
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print('Selecting Best Model: \\n')\n",
    "\n",
    "model_name = models[aucies.index(max(aucies))][0]\n",
    "model = models[aucies.index(max(aucies))][1]\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMQjpaMi_qCu"
   },
   "source": [
    "**********************************************************************************************\n",
    "# 6.Save and Load Machine Learning Models\n",
    "**********************************************************************************************\n",
    "Finding an accurate machine learning model is not the end of the project. We will discover how to save and load our machine learning model in Python using scikit-learn.\n",
    "This allows us to save our model to \f\n",
    "file and load it later in order to make predictions:\n",
    "\n",
    "1. The importance of serializing models for reuse.\n",
    "2. How to use pickle to serialize and deserialize machine learning models.\n",
    "3. How to use Joblib to serialize and deserialize machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfVMpStN_0D0"
   },
   "source": [
    "**********************************************************************************************\n",
    "## 6.1.Finalize Your Model with pickle\n",
    "**********************************************************************************************\n",
    "Pickle is the standard way of serializing objects in Python. You can use the pickle1 operation\n",
    "to serialize our machine learning algorithms and save the serialized format to a \f\n",
    "file. Later we\n",
    "can load this \f\n",
    "file to deserialize our model and use it to make new predictions. The example\n",
    "below demonstrates how we can train a logistic regression model on the Pima Indians onset of\n",
    "diabetes dataset, save the model to fi\f\n",
    "le and load it to make predictions on the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1576210788210,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "Dt0I_r-c_rNy",
    "outputId": "9e7908dc-3a9c-4522-a0f8-eb594796b552"
   },
   "outputs": [],
   "source": [
    "# Training the Algorithm on selected model\n",
    "##############################################################################\n",
    "# print(models)\n",
    "# models[2][1]\n",
    "\n",
    "# model = models[1][1] #SVC(gamma='auto')\n",
    "# The fit method of SVC class is called to train the \n",
    "# algorithm on the training data, which is passed as a parameter to the fit method\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "from pickle import dump\n",
    "##############################################################################\n",
    "\n",
    "#Save Model to disk\n",
    "filename = model_name + '_finalized_model.sav'\n",
    "dump(model, open(filename, 'wb'))\n",
    "print(model_name, ' Model Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDYYHqZ4AEN8"
   },
   "source": [
    "**********************************************************************************************\n",
    "## 6.2Make Predictions: Precision,recall,F1score for all models\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26pPX-hoAFP_"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "from pickle import load\n",
    "##############################################################################\n",
    "\n",
    "loaded_model = load(open(filename, 'rb'))\n",
    "\n",
    "# Running the example saves the model to finalized model.sav in our local working\n",
    "# directory. Load the saved model and evaluating it provides an estimate of accuracy of the model\n",
    "# on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1576214132867,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "DcuZYrAlljVG",
    "outputId": "238554ea-a2b0-45ba-e199-3c40ab1f200b"
   },
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print(\"Prediction on Validation data:\\n\")\n",
    "Y_pred = loaded_model.predict(X_validation)\n",
    "cm = confusion_matrix(Y_validation,Y_pred)\n",
    "print(classification_report(Y_validation,Y_pred, target_names=None))\n",
    "print('Accuracy of ' , model_name, ' Model : ', accuracy(cm)*100, '%')\n",
    "\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print(\"Prediction on Test data:\\n\")\n",
    "Y_pred = loaded_model.predict(X_test)\n",
    "cm = confusion_matrix(Y_test,Y_pred)\n",
    "print(classification_report(Y_test, Y_pred, target_names=None))\n",
    "print('Accuracy of ' , model_name, ' Model : ', accuracy(cm)*100, '%')\n",
    "\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print(\"Prediction on Train data:\\n\")\n",
    "Y_pred = loaded_model.predict(X_train)\n",
    "cm = confusion_matrix(Y_train,Y_pred)\n",
    "print(classification_report(Y_train, Y_pred, target_names=None))\n",
    "print('Accuracy of ' , model_name, ' Model : ', accuracy(cm)*100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cyboEfeKlYPV"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v4yTs8LbAYrz"
   },
   "outputs": [],
   "source": [
    "# Making Predictions on Validation Data\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print(\"Prediction on Validation data:\\n\")\n",
    "\n",
    "# Evaluating the Algorithm\n",
    "# Confusion matrix, precision, recall, and F1 measures are the most commonly \n",
    "# used metrics for classification tasks. Scikit-Learn's metrics \n",
    "# library contains the classification_report and confusion_matrix methods, \n",
    "# which can be readily used to find out the values for these important metrics.\n",
    "# \n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics \n",
    "\n",
    "##############################################################################\n",
    "\n",
    "Y_pred = loaded_model.predict(X_validation)\n",
    "cm = confusion_matrix(Y_validation,Y_pred)\n",
    "print(cm)\n",
    "print(classification_report(Y_validation,Y_pred))\n",
    "\n",
    "# Printing the accuracy\n",
    "print('Accuracy of ' , model_name, ' Model : ', accuracy(cm)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 795
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 977,
     "status": "ok",
     "timestamp": 1576214390786,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "ROTbUCKUkWZx",
    "outputId": "08856976-b3bf-4e73-c63d-0d8f3c8203db"
   },
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "Y_pred = loaded_model.predict(X_validation)\n",
    "gbPredictproba = loaded_model.predict_proba(X_validation)[:,1] #for ROC curve\n",
    "gbAccuracy = accuracy_score(Y_validation,Y_pred)\n",
    "roc_score = metrics.roc_auc_score(Y_validation,Y_pred)\n",
    "print(\"\\nModel %s  accuracy is %f\" % (model_name , gbAccuracy))\n",
    "plotRocCurve(gbPredictproba, Y_validation, model_name)#\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(Y_validation,Y_pred, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1576214425417,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "RmszrPPkAhJw",
    "outputId": "146a41ca-0595-4ed2-86f8-65ba2cb48ba4"
   },
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print(\"Prediction on Test data:\\n\")\n",
    "Y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(Y_test,Y_pred)\n",
    "print(cm)\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "# Printing the accuracy\n",
    "print('Accuracy of ' , model_name, ' Model : ', accuracy(cm)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 778
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 878,
     "status": "ok",
     "timestamp": 1576214443564,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "7Bu8VEUwkjT0",
    "outputId": "b4a93422-489e-4dd3-e1a6-d7581c3f0b6e"
   },
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "Y_pred = loaded_model.predict(X_test)\n",
    "gbPredictproba = loaded_model.predict_proba(X_test)[:,1] #for ROC curve\n",
    "gbAccuracy = accuracy_score(Y_test,Y_pred)\n",
    "roc_score = metrics.roc_auc_score(Y_test,Y_pred)\n",
    "print(\"Model %s  accuracy is %f\" % (model_name , gbAccuracy))\n",
    "plotRocCurve(gbPredictproba, Y_test, 'Gradient Boosting')#\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(Y_test,Y_pred, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1576214523251,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "pqpXsLTHpooX",
    "outputId": "7219b00f-bc9e-48d8-9bf9-03767b0126de"
   },
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "print('\\n')\n",
    "print('*' * 80)\n",
    "print(\"Prediction on Train data:\\n\")\n",
    "\n",
    "Y_pred = loaded_model.predict(X_train)\n",
    "cm = confusion_matrix(Y_train,Y_pred)\n",
    "print(cm)\n",
    "print(classification_report(Y_train,Y_pred))\n",
    "\n",
    "# Printing the accuracy\n",
    "print('Accuracy of ' , model_name, ' Model : ', accuracy(cm)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 778
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 997,
     "status": "ok",
     "timestamp": 1576214539981,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "2Qwf5pNup41o",
    "outputId": "24beafd8-164d-4769-8a46-1287fd6f0a6e"
   },
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "Y_pred = loaded_model.predict(X_train)\n",
    "gbPredictproba = loaded_model.predict_proba(X_train)[:,1] #for ROC curve\n",
    "gbAccuracy = accuracy_score(Y_train,Y_pred)\n",
    "roc_score = metrics.roc_auc_score(Y_train,Y_pred)\n",
    "print(\"Model %s  accuracy is %f\" % (model_name , gbAccuracy))\n",
    "plotRocCurve(gbPredictproba, Y_train, 'Gradient Boosting')#\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(Y_train,Y_pred, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8k6v4eTAEytF"
   },
   "source": [
    "**********************************************************************************************\n",
    "## Printing Probabilities\n",
    "**********************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1576214562040,
     "user": {
      "displayName": "Bassam Arnaout",
      "photoUrl": "",
      "userId": "01605761833637310780"
     },
     "user_tz": 300
    },
    "id": "fD18zGEBFBg6",
    "outputId": "378c2967-1bd4-4cc7-986b-8d4349dcae3e"
   },
   "outputs": [],
   "source": [
    "last_5000_data_to_use = data_formulated.tail(5000)\n",
    "\n",
    "no_of_columns = last_5000_data_to_use.shape[1]\n",
    "no_of_attrib = no_of_columns -1\n",
    "\n",
    "array = last_5000_data_to_use.values\n",
    "X_5000 = array[:,0:no_of_attrib]\n",
    "\n",
    "prob = loaded_model.predict_proba(X_train)\n",
    "# print(X_5000.shape)\n",
    "# print(prob.shape)\n",
    "# print(last_5000_data_to_use.shape)\n",
    "\n",
    "\n",
    "df_cali_with_prob = data_to_use.copy()\n",
    "\n",
    "df_cali_with_prob['Prob<=3000'] = prob[:,0]\n",
    "df_cali_with_prob['Prob>3000'] = prob[:,1]\n",
    "\n",
    "print('*' * 80)\n",
    "print('\\nprinting probabilities...\\n')\n",
    "print(df_cali_with_prob)\n",
    "\n",
    "# print('\\n\\nProb<=3000' , '\\t\\tProb>3000')\n",
    "# for index, row in df_cali.iterrows():\n",
    "#     print(row['Prob<=3000'], row['Prob>3000'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DataMiningFinalProject-All.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
